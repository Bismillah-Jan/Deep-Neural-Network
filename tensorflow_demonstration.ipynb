{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Demonstration\n",
    "### By Bismillah Jan\n",
    "* Dataset used: MNIST\n",
    "   - Description:\n",
    "       The MNIST database of handwritten digits, has a training set of 60,000 examples, and a test set of 10,000 examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation (Linux Envoirnment) \n",
    "* \\$ conda create -n tensorflow\n",
    "* \\$ source activate tensorflow\n",
    "    (tensorflow)$  # Your prompt should change\n",
    "    \n",
    "* (tensorflow)\\$ conda install -c conda-forge tensorflow\n",
    "    \n",
    "* or use\n",
    "  -  \\$  conda install -c https://conda.anaconda.org/jjhelmus tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Network_model(input_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    tensorflow models take inputs in the form of dictionary, so lets initialize the weights and biases \n",
    "    as given below.\n",
    "    \"\"\"\n",
    "    layer1_param={'weights':tf.Variable(tf.random_normal([784, no_neurons_layer1])), \n",
    "    'biases': tf.Variable(tf.random_normal([no_neurons_layer1]))}\n",
    "    \n",
    "    layer2_param={'weights':tf.Variable(tf.random_normal([no_neurons_layer1, no_neurons_layer2])), \n",
    "    'biases': tf.Variable(tf.random_normal([no_neurons_layer2]))}\n",
    "    \n",
    "    layer3_param={'weights':tf.Variable(tf.random_normal([no_neurons_layer2, no_neurons_layer3])), \n",
    "    'biases': tf.Variable(tf.random_normal([no_neurons_layer3]))}\n",
    "    \n",
    "    layer4_param={'weights':tf.Variable(tf.random_normal([no_neurons_layer3, no_neurons_layer4])), \n",
    "    'biases': tf.Variable(tf.random_normal([no_neurons_layer4]))}\n",
    "    \n",
    "    output_layer_param={'weights':tf.Variable(tf.random_normal([no_neurons_layer4, no_classes])), \n",
    "    'biases': tf.Variable(tf.random_normal([no_classes]))}\n",
    "    \n",
    "    #so uptill now the weights for each layer is initialized\n",
    "    \n",
    "    \"\"\"\n",
    "    Now what will happened in each layer, I will define next. basically the weights are multiplied\n",
    "    in each layer with the corresponding inputs and then it is passed through activation function \n",
    "    (relu in this case) and the output is given as input to the other layer.\n",
    "    sign:B-Jan\n",
    "    \"\"\"\n",
    "    \n",
    "    l1_output= tf.add(tf.matmul(input_data,layer1_param['weights']), layer1_param['biases'])\n",
    "    l1_output=tf.nn.relu(l1_output)\n",
    "    \n",
    "    l2_output= tf.add(tf.matmul(l1_output,layer2_param['weights']), layer2_param['biases'])\n",
    "    l2_output=tf.nn.relu(l2_output)\n",
    "    \n",
    "    \n",
    "    l3_output= tf.add(tf.matmul(l2_output,layer3_param['weights']), layer3_param['biases'])\n",
    "    l3_output=tf.nn.relu(l3_output)\n",
    "    \n",
    "    l4_output= tf.add(tf.matmul(l3_output,layer4_param['weights']), layer4_param['biases'])\n",
    "    l4_output=tf.nn.relu(l4_output)\n",
    "    \n",
    "    #The final output Layer\n",
    "    output= tf.matmul(l4_output, output_layer_param['weights'])+output_layer_param['biases']\n",
    "    \n",
    "    return output # contains the output of the last output layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(x, epochs):\n",
    "    \n",
    "    # the previous function is called\n",
    "    model_prediction= Network_model(x)\n",
    "    \n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=model_prediction, labels=y) )\n",
    "    \n",
    "    #using built in Adam Algorithm to minimize the cost                      \n",
    "    optimizer=tf.train.AdamOptimizer().minimize(cost) \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            loss=0\n",
    "            for k in range(int(mnist.train.num_examples/batch_size)):\n",
    "                xx,yy = mnist.train.next_batch(batch_size)\n",
    "                l, c= sess.run([optimizer, cost], feed_dict={x:xx, y:yy})\n",
    "                loss=loss+c \n",
    "            print \"Epoch: \", i+1, \"/\", epochs, \"  Loss is : \", loss    \n",
    "            \n",
    "        #Here I want to test the model on the test data    \n",
    "        #staying in the same session, otherwise it will rise an error        \n",
    "        evaluate= tf.equal(tf.argmax(model_prediction, 1), tf.arg_max(y, 1))\n",
    "        accuracy=tf.reduce_mean(tf.cast(evaluate, 'float'))\n",
    "        print \"Accuracy is :\", accuracy.eval({x:mnist.test.images, y:mnist.test.labels})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch:  1 / 5   Loss is :  2783908.50555\n",
      "Epoch:  2 / 5   Loss is :  726657.206848\n",
      "Epoch:  3 / 5   Loss is :  446836.248322\n",
      "Epoch:  4 / 5   Loss is :  311062.80126\n",
      "Epoch:  5 / 5   Loss is :  229402.654243\n",
      "Accuracy is : 0.8846\n"
     ]
    }
   ],
   "source": [
    "DataPath=\"/tmp/data\"\n",
    "mnist= input_data.read_data_sets(DataPath, one_hot=True)\n",
    "\n",
    "#Going to create a 4 Hidden layers model\n",
    "no_neurons_layer1= 100\n",
    "no_neurons_layer2= 200\n",
    "no_neurons_layer3= 200\n",
    "no_neurons_layer4=100\n",
    "\n",
    "no_classes=10\n",
    "batch_size=100\n",
    "epochs=5\n",
    "\n",
    "\n",
    "x= tf.placeholder('float',[None, 784] )\n",
    "y= tf.placeholder('float')\n",
    "\"\"\"\n",
    "784 is due the size of the image 28x28 the mnist dataset contains images of size 28x28=784 pixels\n",
    "these 784 pixels will be given as input to the model in flate array\n",
    "\n",
    "\"\"\"\n",
    "#Training Process starts\n",
    "train_model(x, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
